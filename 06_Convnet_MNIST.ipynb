{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional networks\n",
    "\n",
    "A basic convnet is a stack of ```Conv2D``` and ```MaxPooling2D``` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- The output of every ```Conv2D``` and ```MaxPooling2D``` layer is a 3D tensor of shpae (```height```, ```width```, ```channels```).\n",
    "\n",
    "- ```width``` and ```height``` dimensions tend to shrink as the network goes deeper.\n",
    "\n",
    "- The number of channels is controled by the first argument passed to the ```Conv2D``` layers (32 or 64).\n",
    "\n",
    "**Next step: add a classifier on top of the convnet**\n",
    "\n",
    "Feed the last output tensor into a densely connected classifier network.\n",
    "But first, we have to flatten the 3D outputs to 1D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a 10-way (10 digits of the MNIST dataset) classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train the convnet on the MNIST digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A densely connected network has a test accuracy of aprox 97.8%, the basic convet goes up to 99.3%\n",
    "\n",
    "## The convolution operation\n",
    "```Dense``` layers learn **global** patterns in their input feature space.\n",
    "\n",
    "-> Convolution layers learn **local** patterns. In the case of images, patterns found in small 2D windows of the inputs. In the previous example, these windows were all 3x3.\n",
    "\n",
    "This gives convnets two properties:\n",
    "- ***The patterns they learn are translation invariant:*** So after learning a certain pattern in the lower-right corner of a picture, it can recognize it anywhere.\n",
    "- ***They can learn spatial hierarchies of patterns:*** They efficiently leanr increasingly complex and abstract visual concepts (so a first conv layer will learn small local patterns as edges, and the second will learn larger patterns made of features of the first layer, and so on.)\n",
    "\n",
    "Convolutions operate over 3D tensors, called *feature maps*, with two spatial axes (```height``` and ```width```) as well as a ```depth``` axis (also called the *channels* axis). So, an RGB image has ```depth = 3``` (red, green, and blue). A black-and-white picture, ```depth = 1``` (levels of gray).\n",
    "\n",
    "The convolution operation results on an *output feature map* which is still a 3D tensor with ```width``` and ```height```, and ```depth``` but here it doesn't stand for colors, it stands for *filters*.\n",
    "\n",
    "Convolutions are defined by two key parameters:\n",
    "- ***Size of the patches extracted from the inputs:***  typically 3 × 3 or 5 × 5.\n",
    "- ***Depth of the output feature map:***  the number of filters computed.\n",
    "\n",
    "##### Why can the output width and height be different from the input width and heigth?\n",
    "=> **BORDER EFFECTS AND PADDING**\n",
    "In Conv2D layers, padding is configurable via the padding argument, which can be: \n",
    "- \"valid\", which means no padding (only valid window locations will be used), so the output will \"shrink\" (border effect)\n",
    "- \"same\", which means “pad in such a way as to have an output with the same width and height as the input.”\n",
    "\n",
    "=> **STRIDES**\n",
    "Rarely used, but basically the *stride* is the distance between two successive windows, which defaults to 1. If its set to be higher than 1, the width and height of the feature map are downsampled by the same factor of the stride (plus border effects). \n",
    "\n",
    "#### Max-pooling operation\n",
    "The role of max pooling: to aggressively downsample feature maps, much like\n",
    "strided convolutions. It consists of extracting windows from the input feature maps and outputting the max value of each channel.\n",
    "\n",
    "Usually done with 2 x 2 windows and stride 2 => downsample by a factor of 2.\n",
    "\n",
    "**Why use max-pooling?**\n",
    "- reduce the number of feature-map coefficients to process\n",
    "- induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
