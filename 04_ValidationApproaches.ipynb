{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple hold-out validation\n",
    "Simply set appart some fraction of the data set as the test set. Train on the remaining data and evaluate on the test set. \n",
    "\n",
    "IMPORTANT: to prevent *information leaks* to the NN, the model mustn't be tuned based on the test set, so a good practice is to *also* reserve a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle is usually appropriate (except when time is an important feature!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines validation and training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data[:num_validation_samples]\n",
    "data = data[num_validation_samples:]\n",
    "\n",
    "training_data = data[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains the model on the training data, and evaluates it on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "# At this point you can tune your model,\n",
    "# retrain it, evaluate it, tune it again...\n",
    "# using, as discussed, the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the hyperparameters are turned, it's common to train the final model from scratch on all non-test data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.train(np.concatenate([training_data,\n",
    "                        validation_data]))\n",
    "\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "This is a very simple evaluation protocol that suffers from one flaw: if little data is available, the the validation and test sets may contain too few examples. This problem is easy to stop: if different random shuffling round of the data before splitting end up yielding very different measures of model performance, is safe to assume the sets are too small. The solutions are discussed next:\n",
    "\n",
    "# K-fold validation\n",
    "\n",
    "The steps as:\n",
    "- split your data into K partitions of equal size. \n",
    "- For each partition i , train a model on the remaining K â€“ 1 partitions, and evaluate it on partition i .\n",
    "- Calculate the final score as the averages of the K scores obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_validation_samples = len(data) // k\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_scores = []\n",
    "for fold in range(k):\n",
    "    validation_data = data[num_validation_samples * fold:\n",
    "    num_validation_samples * (fold + 1)]\n",
    "    training_data = data[:num_validation_samples * fold] +\n",
    "        data[num_validation_samples * (fold + 1):]\n",
    "        \n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "\n",
    "validation_score = np.average(validation_scores)\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterated K-fold valdiation with shuffling\n",
    "When there's relatively little data available, a good practice is to apply K-fold validation multiple times, shuffling the data every time *before* splitting in *K* ways.\n",
    "\n",
    "The final score is the average of the scores obtained at each run of K-fold validation.\n",
    "\n",
    "This can be expensive, as it involves training and evaluation *P x K* models (where *P* is the number of iterations used).\n",
    "\n",
    "**IMPORTANT:** If the objective is to predict the future given the past, we must *NOT* randomly shuffle the data before splitting -> temporal leak!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
