{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing what convnets learn\n",
    "The representations learned by convnets are highly amenable to visualization because they're *representations of visual concepts*.\n",
    "\n",
    "- Visualizing intermediate activations\n",
    "- Visualizing convnets filters\n",
    "- Visualizing heatmaps of class activation in an image\n",
    "\n",
    "## Visualizing intermediate activations\n",
    "It's displaying the feature maps that are output by various convolution and pooling layers in a network, given a certain input (the output of a layer is often called its *activation*). \n",
    "This allows us to see how an input is decomposed into the filters learned by the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('/tf/data/saved-models/cats_and_dogs_small_2.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an input image from the test set (not part of the images the network was trained on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/tf/data/test/cats/cat.1700.jpg'\n",
    "\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "img_tensor /= 255.\n",
    "\n",
    "print(img_tensor.shape)\n",
    "\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to etract the feaure maps, create a Keras model that takes batches of images as input and outputs the activations of all convolution and pooling layers. \n",
    "Use the ```Model``` class instead of ```Sequential``` class because the first one allows for models with **multiple outputs**, unlike the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fed an image input, this model returns the values of the layer activations in the original model. The output for this model has one input (the image) and 8 outputs (one per layer activation).\n",
    "\n",
    "For instance, this is the activation of the first convolution layer for the cat image input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_model.predict(img_tensor)\n",
    "\n",
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a 148 x 148 with 32 channels (can verify this with ```model.summary()```). If we plot the *fourth channel* of the activation of the first layer of the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(first_layer_activation[0, :, :, 7], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific filters learned by convolution layers aren't deterministic, so they may vary from model to model, but some examples of filters are *diagonal edge detector* or *bright green dot*, etc..\n",
    "\n",
    "We can also extract and plot every channel in each of the 8 activation maps and stack the results in one big image tensor, with channels stacked side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the layers, to ahve them as part of the plot\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "# Define size of big image tensor\n",
    "images_per_row = 16\n",
    "\n",
    "# Display the feature maps\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    #Number of features in the feature map\n",
    "    n_features = layer_activation.shape[-1]\n",
    "    \n",
    "    #The feature map has shape (1, size, size, n_features) \n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # Tiles of the activation channels in this matrix\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- The **first layer** acts as a collection of various **edge detectors**. The activations retain almost all of the information present in the initial picture.\n",
    "- As you go **higher**, the activations become increasingly **abstract** and less visually interpretable. They begin to encode **higher-level concepts** such as “cat ear” and “cat eye.” Higher presentations carry increasingly **less information about the visual contents of the image**, and increasingly **more information related to the class of the image**.\n",
    "- The sparsity of the activations increases with the depth of the layer: in the first layer, all filters are activated by the input image; but in the **higher layers, more and more filters are blank** => This means the pattern encoded by the filter isn’t found in the input image.\n",
    "\n",
    "## Visualizing convnet filters\n",
    "Another way of inspecting the filters learned by convnets is to display the visual pattern that each filter is meant to respond to. This can be done with **gradient ascent in input space** : applying **gradient descent** to the value of the input image of a convnet so as to *maximize* the response of a specific filter, starting from a blank input image. \n",
    "\n",
    "The resulting input image will be one that the chosen filter is maximally responsive to.\n",
    "\n",
    "- Build a loss function that maximizes the value of a given filter in a given convolution layer\n",
    "- Use stochastic gradient descent to adjust the values of the input image so as to maximize this activation value\n",
    "\n",
    "**EXAMPLE:** loss for the activation of filter 0 in the layer block3_conv1 of the VGG16 network, pretrained on ImageNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "model = VGG16(weights='imagenet',\n",
    "            include_top=False)\n",
    "\n",
    "layer_name = 'block3_conv1'\n",
    "filter_index = 0\n",
    "\n",
    "layer_output = model.get_layer(layer_name).output\n",
    "loss = K.mean(layer_output[:, :, :, filter_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement gradient descent, we need the gradient of this loss with respect to the model’s input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with tf.GradientTape() as gtape:\n",
    "    grads = gtape.gradient(loss, model.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A non-obvious trick to use to help the gradient-descent process go smoothly is to normalize the gradient tensor by dividing it by its L2 norm (the square root of the average of the square of the values in the tensor) => so the magnitude of the updates done to the input image is always within the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing heatmaps of class activation\n",
    "This is useful for understanding which parts of a given image led a convnet to its final classification decision.\n",
    "- Why did the network think the image contained the choseen class?\n",
    "- Where is the class located in the picture?\n",
    "\n",
    "This is called *class activation map (CAM)* and consists of producing heatmaps of class activation over input images. A *class activation heatmap* is a 2D grid of scores associated with a specific output class, computed for every location in any input image, indicating how important each location is.\n",
    "\n",
    "Intuitively, is weightinf a spacial map of “how intensely the input image **activates different channels**” by “**how important each channel is** with regard to the class,” resulting in:\n",
    "***a spatial map of “how intensely the input image activates the class.”***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16(weights='imagenet') # include the densely connected classifier on top, previously discarded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this, we must convert any image into something VGG16 model can read:\n",
    "- resize to 224x224\n",
    "- convert to a Numpy ```float32``` tensor\n",
    "- Apply preprocessing rules (```keras.applications.vgg16.preprocess_input```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "img_path = '.......'\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up the Grand-CAM algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x)\n",
    "\n",
    "\n",
    "african_e66lephant_output = model.output[:, np.argmax(preds[0])]\n",
    "\n",
    "last_conv_layer = model.get_layer('block5_conv3')\n",
    "\n",
    "# This line doesn't work!\n",
    "grads = K.gradients(african_elephant_output, last_conv_layer.output)[0]\n",
    "\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "\n",
    "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "\n",
    "for i in range(512):\n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "    \n",
    "heatmap = np.mean(conv_layer_output_value, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heatmap post-processing** -> normalize the heatma between 0 and 1 (visualizarion purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **OpenCV** to generate an image taht superimposes the original image on the heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path)\n",
    "\n",
    "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "superimposed_img = heatmap * 0.4 + img\n",
    "\n",
    "cv2.imwrite('......./elephant_cam.jpg', superimposed_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
